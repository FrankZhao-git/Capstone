{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0o300RepBA2"
   },
   "source": [
    "# paltform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpN6az3BmU5C"
   },
   "outputs": [],
   "source": [
    "!pip install textworld\n",
    "!pip install textworld[vis]\n",
    "!pip install gym\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cM8g_XT4o9Ox"
   },
   "source": [
    "# LSTM-DRQN code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmCxbwzu4ymS"
   },
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxQUR3RErJMQ",
    "outputId": "485c549f-53e7-4566-92f4-3a4f3afe65d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aJDNMDkCqoCH"
   },
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "import yaml\n",
    "config_file = pjoin('./capstone/', 'config_drqn.yaml')\n",
    "with open(config_file) as reader:\n",
    "    config = yaml.safe_load(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhwybjVuK7Qx"
   },
   "source": [
    "## Make Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FlfU-C8K-j7"
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import _shell\n",
    "_shell.Shell.run_code\n",
    "def makegamescmd(num,level):\n",
    "    pattern='tw-make tw-coin_collector --level {} --seed {} --save-overview  --output /content/drive/My\\ Drive/capstone/tw_games/twcc_easy_level{}_gamesize100_step50_seed{}_train-v0.ulx'\n",
    "    games_files=[]\n",
    "    start=1\n",
    "    for i in range(start,num+start):\n",
    "        gamename=pattern.format(level,i,level,i)\n",
    "        # ! $gamename\n",
    "        games_files.append('twcc_easy_level{}_gamesize100_step50_seed{}_train-v0.ulx'.format(level,i))\n",
    "    return games_files\n",
    "games_files=makegamescmd(100,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAvLpaRVwL_y"
   },
   "outputs": [],
   "source": [
    "from google.colab import _shell\n",
    "# from  google.colab import invokeFunction\n",
    "_shell.Shell.run_code\n",
    "def makegamescmd_TEST(num,level):\n",
    "    pattern='tw-make tw-coin_collector --level {} --seed {} --save-overview  --output /content/drive/My\\ Drive/capstone/tw_games_test/twcc_easy_level{}_gamesize100_step50_seed{}_train-v0.ulx'\n",
    "    games_files=[]\n",
    "    start=1000\n",
    "    for i in range(start,num+start):\n",
    "        gamename=pattern.format(level,i,level,i)\n",
    "        # ! $gamename\n",
    "        games_files.append('twcc_easy_level{}_gamesize100_step50_seed{}_train-v0.ulx'.format(level,i))\n",
    "    return games_files\n",
    "TEST_games_files=makegamescmd_TEST(20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUCgy8vDv-iB"
   },
   "outputs": [],
   "source": [
    "\n",
    "def testing(env_test,batch_size_test,agent):\n",
    "\n",
    "    obs, infos = env_test.reset()\n",
    "    obs=list(obs)\n",
    "    agent.reset(infos)\n",
    "\n",
    "    print_command_string, print_rewards = [[] for _ in range(batch_size_test)], [[] for _ in range(batch_size_test)]\n",
    "    print_interm_rewards = [[] for _ in range(batch_size_test)]\n",
    "\n",
    "    provide_prev_action = config['general']['provide_prev_action']\n",
    "\n",
    "    dones = [False] * batch_size_test\n",
    "    rewards = None\n",
    "    prev_actions = [\"\" for _ in range(batch_size_test)] if provide_prev_action else None\n",
    "\n",
    "\n",
    "    input_description, _ = agent.get_game_step_info(obs, infos, prev_actions)\n",
    "\n",
    "\n",
    "    curr_ras_hidden, curr_ras_cell = None, None\n",
    "\n",
    "    while not all(dones):\n",
    "\n",
    "        v_idx, n_idx, chosen_strings, curr_ras_hidden, curr_ras_cell = agent.generate_one_command(input_description, curr_ras_hidden, curr_ras_cell, epsilon=0.0)\n",
    "        # print(chosen_strings)\n",
    "        obs, rewards, dones, infos = env_test.step(chosen_strings)\n",
    "        obs=list(obs)\n",
    "        rewards=list(rewards)\n",
    "        dones=list(dones)\n",
    "\n",
    "\n",
    "\n",
    "        if provide_prev_action:\n",
    "            prev_actions = chosen_strings\n",
    "\n",
    "        for i in range(batch_size_test):\n",
    "            print_command_string[i].append(chosen_strings[i])\n",
    "            print_rewards[i].append(rewards[i])\n",
    "            print_interm_rewards[i].append(infos[\"intermediate_reward\"][i])\n",
    "        if type(dones) is bool:\n",
    "            dones = [dones] * batch_size\n",
    "        agent.rewards.append(rewards)\n",
    "        agent.dones.append(dones)\n",
    "        agent.intermediate_rewards.append(infos[\"intermediate_reward\"])\n",
    "\n",
    "        input_description, _ = agent.get_game_step_info(obs, infos, prev_actions)\n",
    "\n",
    "    agent.finish()\n",
    "    R = agent.final_rewards.mean()\n",
    "    S = agent.step_used_before_done.mean()\n",
    "    IR = agent.final_intermediate_rewards.mean()\n",
    "\n",
    "    msg = '====EVAL==== R={:.3f}, IR={:.3f}, S={:.3f}'\n",
    "    msg = msg.format(R, IR, S)\n",
    "    print(msg)\n",
    "    print(\"\\n\")\n",
    "    return (R, IR, S)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hasyceyh41bm"
   },
   "source": [
    "## dict2vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1wpS52e43_N"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from textworld.utils import maybe_mkdir\n",
    "import time\n",
    "\n",
    "\n",
    "class SlidingAverage(object):\n",
    "    def __init__(self, name, steps=100):\n",
    "        self.name = name\n",
    "        self.steps = steps\n",
    "        self.t = 0\n",
    "        self.ns = []\n",
    "        self.avgs = []\n",
    "\n",
    "    def add(self, n):\n",
    "        self.ns.append(n)\n",
    "        if len(self.ns) > self.steps:\n",
    "            self.ns.pop(0)\n",
    "        self.t += 1\n",
    "        if self.t % self.steps == 0:\n",
    "            self.avgs.append(self.value)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if len(self.ns) == 0: return 0\n",
    "        return sum(self.ns) / len(self.ns)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%s=%.4f\" % (self.name, self.value)\n",
    "\n",
    "    def __gt__(self, value): return self.value > value\n",
    "    def __lt__(self, value): return self.value < value\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'t': self.t,\n",
    "                'ns': tuple(self.ns),\n",
    "                'avgs': tuple(self.avgs)}\n",
    "\n",
    "    def load_state_dict(self, state):\n",
    "        self.t = state[\"t\"]\n",
    "        self.ns = list(state[\"ns\"])\n",
    "        self.avgs = list(state[\"avgs\"])\n",
    "\n",
    "\n",
    "def to_np(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def to_pt(np_matrix, enable_cuda=False, type='long'):\n",
    "    if type == 'long':\n",
    "        if enable_cuda:\n",
    "            return torch.autograd.Variable(torch.from_numpy(np_matrix).type(torch.LongTensor).cuda())\n",
    "        else:\n",
    "            return torch.autograd.Variable(torch.from_numpy(np_matrix).type(torch.LongTensor))\n",
    "    elif type == 'float':\n",
    "        if enable_cuda:\n",
    "            return torch.autograd.Variable(torch.from_numpy(np_matrix).type(torch.FloatTensor).cuda())\n",
    "        else:\n",
    "            return torch.autograd.Variable(torch.from_numpy(np_matrix).type(torch.FloatTensor))\n",
    "\n",
    "\n",
    "def get_experiment_dir(config,env_id='twcc_easy_level5_gamesize100',info='*'):\n",
    "    # env_id = config['general']['env_id']\n",
    "    exps_dir = config['general']['experiments_dir'] \n",
    "    exp_tag = config['general']['experiment_tag']+'_'+ info + '_' + time.strftime(\"%Y-%m-%d_%H:%M:%S\", time.localtime())\n",
    "    exp_dir = pjoin(exps_dir, env_id + \"_\" + exp_tag)\n",
    "    return maybe_mkdir(exp_dir)\n",
    "\n",
    "\n",
    "def dict2list(id2w_dict):\n",
    "    res = []\n",
    "    for item in id2w_dict:\n",
    "        res.append(item)\n",
    "    return res\n",
    "\n",
    "\n",
    "def _words_to_ids(words, word2id):\n",
    "    ids = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            ids.append(word2id[word])\n",
    "        except KeyError:\n",
    "            ids.append(1)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def preproc(s, str_type='None', lower_case=False):\n",
    "    s = s.replace(\"\\n\", ' ')\n",
    "    if s.strip() == \"\":\n",
    "        return [\"nothing\"]\n",
    "    if str_type == 'description':\n",
    "        s = s.split(\"=-\")[1]\n",
    "    elif str_type == 'inventory':\n",
    "        s = s.split(\"carrying\")[1]\n",
    "        if s[0] == ':':\n",
    "            s = s[1:]\n",
    "    elif str_type == 'feedback':\n",
    "        if \"Welcome to Textworld\" in s:\n",
    "            s = s.split(\"Welcome to Textworld\")[1]\n",
    "        if \"-=\" in s:\n",
    "            s = s.split(\"-=\")[0]\n",
    "    s = s.strip()\n",
    "    if len(s) == 0:\n",
    "        return [\"nothing\"]\n",
    "    tokens = wt(s)\n",
    "    if lower_case:\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def max_len(list_of_list):\n",
    "    return max(map(len, list_of_list))\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.):\n",
    "    '''\n",
    "    FROM KERAS\n",
    "    Pads each sequence to the same length:\n",
    "    the length of the longest sequence.\n",
    "    If maxlen is provided, any sequence longer\n",
    "    than maxlen is truncated to maxlen.\n",
    "    Truncation happens off either the beginning (default) or\n",
    "    the end of the sequence.\n",
    "    Supports post-padding and pre-padding (default).\n",
    "    # Arguments\n",
    "        sequences: list of lists where each element is a sequence\n",
    "        maxlen: int, maximum length\n",
    "        dtype: type to cast the resulting sequence.\n",
    "        padding: 'pre' or 'post', pad either before or after each sequence.\n",
    "        truncating: 'pre' or 'post', remove values from sequences larger than\n",
    "            maxlen either in the beginning or in the end of the sequence\n",
    "        value: float, value to pad the sequences to the desired value.\n",
    "    # Returns\n",
    "        x: numpy array with dimensions (number_of_sequences, maxlen)\n",
    "    '''\n",
    "    lengths = [len(s) for s in sequences]\n",
    "\n",
    "    nb_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if len(s) == 0:\n",
    "            continue  # empty list was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkZ2e2EW-F_o"
   },
   "source": [
    "## FastUniGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qn3sdDFY-FLu"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FastUniGRU(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/facebookresearch/DrQA/\n",
    "    now supports:   different rnn size for each layer\n",
    "                    all zero rows in batch (from time distributed layer, by reshaping certain dimension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ninp, nhids, dropout_between_rnn_layers=0.):\n",
    "        super(FastUniGRU, self).__init__()\n",
    "        self.ninp = ninp\n",
    "        self.nhids = nhids\n",
    "        self.nlayers = len(self.nhids)\n",
    "        self.dropout_between_rnn_layers = dropout_between_rnn_layers\n",
    "        self.stack_rnns()\n",
    "\n",
    "    def stack_rnns(self):\n",
    "        rnns = [torch.nn.GRU(self.ninp if i == 0 else self.nhids[i - 1],\n",
    "                              self.nhids[i],\n",
    "                              num_layers=1,\n",
    "                              bidirectional=False) for i in range(self.nlayers)]\n",
    "        self.rnns = torch.nn.ModuleList(rnns)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        def pad_(tensor, n):\n",
    "            if n > 0:\n",
    "                zero_pad = torch.autograd.Variable(torch.zeros((n,) + tensor.size()[1:]))\n",
    "                if x.is_cuda:\n",
    "                    zero_pad = zero_pad.cuda()\n",
    "                tensor = torch.cat([tensor, zero_pad])\n",
    "            return tensor\n",
    "\n",
    "        \"\"\"\n",
    "        inputs: x:          batch x time x inp\n",
    "                mask:       batch x time\n",
    "        output: encoding:   batch x time x hidden[-1]\n",
    "        \"\"\"\n",
    "        # Compute sorted sequence lengths\n",
    "        batch_size = x.size(0)\n",
    "        lengths = mask.data.eq(1).long().sum(1)  # .squeeze()\n",
    "        _, idx_sort = torch.sort(lengths, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "\n",
    "        lengths = list(lengths[idx_sort])\n",
    "        idx_sort = torch.autograd.Variable(idx_sort)\n",
    "        idx_unsort = torch.autograd.Variable(idx_unsort)\n",
    "\n",
    "        # Sort x\n",
    "        x = x.index_select(0, idx_sort)\n",
    "\n",
    "        # remove non-zero rows, and remember how many zeros\n",
    "        n_nonzero = np.count_nonzero(lengths)\n",
    "        n_zero = batch_size - n_nonzero\n",
    "        if n_zero != 0:\n",
    "            lengths = lengths[:n_nonzero]\n",
    "            x = x[:n_nonzero]\n",
    "\n",
    "        # Transpose batch and sequence dims\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # Pack it up\n",
    "        rnn_input = torch.nn.utils.rnn.pack_padded_sequence(x, lengths)\n",
    "\n",
    "        # Encode all layers\n",
    "        outputs = [rnn_input]\n",
    "        for i in range(self.nlayers):\n",
    "            rnn_input = outputs[-1]\n",
    "\n",
    "            # dropout between rnn layers\n",
    "            if self.dropout_between_rnn_layers > 0:\n",
    "                dropout_input = F.dropout(rnn_input.data,\n",
    "                                          p=self.dropout_between_rnn_layers,\n",
    "                                          training=self.training)\n",
    "                rnn_input = torch.nn.utils.rnn.PackedSequence(dropout_input,\n",
    "                                                              rnn_input.batch_sizes)\n",
    "            seq, last = self.rnns[i](rnn_input)\n",
    "            outputs.append(seq)\n",
    "            if i == self.nlayers - 1:\n",
    "                # last layer\n",
    "                last_state = last[0]  # (num_layers * num_directions, batch, hidden_size)\n",
    "                last_state = last_state[0]  # batch x hidden_size\n",
    "\n",
    "        # Unpack everything\n",
    "        for i, o in enumerate(outputs[1:], 1):\n",
    "            outputs[i] = torch.nn.utils.rnn.pad_packed_sequence(o)[0]\n",
    "        output = outputs[-1]\n",
    "\n",
    "        # Transpose and unsort\n",
    "        output = output.transpose(0, 1)  # batch x time x enc\n",
    "\n",
    "        # re-padding\n",
    "        output = pad_(output, n_zero)\n",
    "        last_state = pad_(last_state, n_zero)\n",
    "\n",
    "        output = output.index_select(0, idx_unsort)\n",
    "        last_state = last_state.index_select(0, idx_unsort)\n",
    "\n",
    "        # Pad up to original batch sequence length\n",
    "        if output.size(1) != mask.size(1):\n",
    "            padding = torch.zeros(output.size(0),\n",
    "                                  mask.size(1) - output.size(1),\n",
    "                                  output.size(2)).type(output.data.type())\n",
    "            output = torch.cat([output, torch.autograd.Variable(padding)], 1)\n",
    "\n",
    "        output = output.contiguous() * mask.unsqueeze(-1)\n",
    "        return output, last_state, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EV4uKgZDSzsU"
   },
   "source": [
    "## LSTM-DRQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8Tid4dNUO1E"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('./capstone/')\n",
    "\n",
    "from helpers.layers import Embedding, masked_mean, LSTMCell, FastUniLSTM\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LSTM_DQN(torch.nn.Module):\n",
    "    model_name = 'lstm_dqn'\n",
    "\n",
    "    def __init__(self, model_config, word_vocab, verb_map, noun_map, enable_cuda=False):\n",
    "        super(LSTM_DQN, self).__init__()\n",
    "        self.model_config = model_config\n",
    "        self.enable_cuda = enable_cuda\n",
    "        self.word_vocab_size = 1000  ##len(word_vocab)  #cant get the ob_space ??? need to solved\n",
    "        self.id2word = word_vocab\n",
    "        self.n_actions = 2  #len(verb_map)\n",
    "        self.n_objects = 5  #len(noun_map)\n",
    "        self.read_config()\n",
    "        self._def_layers()\n",
    "        self.init_weights()\n",
    "        # self.print_parameters()\n",
    "\n",
    "    def print_parameters(self):\n",
    "        amount = 0\n",
    "        for p in self.parameters():\n",
    "            amount += np.prod(p.size())\n",
    "        print(\"total number of parameters: %s\" % (amount))\n",
    "        parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        amount = 0\n",
    "        for p in parameters:\n",
    "            amount += np.prod(p.size())\n",
    "        print(\"number of trainable parameters: %s\" % (amount))\n",
    "\n",
    "    def read_config(self):\n",
    "        # model config\n",
    "        config = self.model_config[self.model_name]\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.encoder_rnn_hidden_size = config['encoder_rnn_hidden_size']\n",
    "        self.action_scorer_hidden_dim = config['action_scorer_hidden_dim']\n",
    "        self.dropout_between_rnn_layers = config['dropout_between_rnn_layers']\n",
    "\n",
    "    def _def_layers(self):\n",
    "\n",
    "        # word embeddings\n",
    "        self.word_embedding = Embedding(embedding_size=self.embedding_size,\n",
    "                                        vocab_size=self.word_vocab_size,\n",
    "                                        enable_cuda=self.enable_cuda)\n",
    "\n",
    "        # gru encoder\n",
    "        self.encoder = FastUniGRU(ninp=self.embedding_size,\n",
    "                                   nhids=self.encoder_rnn_hidden_size,\n",
    "                                   dropout_between_rnn_layers=self.dropout_between_rnn_layers)\n",
    "\n",
    "        # Recurrent network for temporal dependencies (a.k.a history).\n",
    "\n",
    "        self.action_scorer_shared_recurrent = LSTMCell(input_size=self.encoder_rnn_hidden_size[-1],\n",
    "                                                       hidden_size=self.action_scorer_hidden_dim)\n",
    "\n",
    "        self.action_scorer_shared = torch.nn.Linear(self.encoder_rnn_hidden_size[-1], self.action_scorer_hidden_dim)\n",
    "        self.action_scorer_action = torch.nn.Linear(self.action_scorer_hidden_dim, self.n_actions, bias=False)\n",
    "        self.action_scorer_object = torch.nn.Linear(self.action_scorer_hidden_dim, self.n_objects, bias=False)\n",
    "        self.fake_recurrent_mask = None\n",
    "\n",
    "    def init_weights(self):\n",
    "        torch.nn.init.xavier_uniform_(self.action_scorer_shared.weight.data, gain=1)\n",
    "        torch.nn.init.xavier_uniform_(self.action_scorer_action.weight.data, gain=1)\n",
    "        torch.nn.init.xavier_uniform_(self.action_scorer_object.weight.data, gain=1)\n",
    "        self.action_scorer_shared.bias.data.fill_(0)\n",
    "\n",
    "    def representation_generator(self, _input_words):\n",
    "        embeddings, mask = self.word_embedding.forward(_input_words)  # batch x time x emb\n",
    "        encoding_sequence, _, _ = self.encoder.forward(embeddings, mask)  # batch x time x h\n",
    "        mean_encoding = masked_mean(encoding_sequence, mask)  # batch x h\n",
    "        return mean_encoding\n",
    "\n",
    "    def recurrent_action_scorer(self, state_representation, last_hidden=None, last_cell=None):\n",
    "        # state representation: batch x input\n",
    "        # last hidden / last cell: batch x hid\n",
    "        if self.fake_recurrent_mask is None or self.fake_recurrent_mask.size(0) != state_representation.size(0):\n",
    "            self.fake_recurrent_mask = torch.autograd.Variable(torch.ones(state_representation.size(0),))\n",
    "            if self.enable_cuda:\n",
    "                self.fake_recurrent_mask = self.fake_recurrent_mask.cuda()\n",
    "\n",
    "        new_h, new_c = self.action_scorer_shared_recurrent.forward(state_representation, self.fake_recurrent_mask, last_hidden, last_cell)\n",
    "        action_rank = self.action_scorer_action.forward(new_h)  # batch x n_action\n",
    "        object_rank = self.action_scorer_object.forward(new_h)  # batch x n_object\n",
    "        return action_rank, object_rank, new_h, new_c\n",
    "\n",
    "    def action_scorer(self, state_representation):\n",
    "        hidden = self.action_scorer_shared.forward(state_representation)  # batch x hid\n",
    "        hidden = F.relu(hidden)  # batch x hid\n",
    "        action_rank = self.action_scorer_action.forward(hidden)  # batch x n_action\n",
    "        object_rank = self.action_scorer_object.forward(hidden)  # batch x n_object\n",
    "        return action_rank, object_rank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbDBCPMOypFO"
   },
   "source": [
    "## RLagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBoN-o_N8Ca_",
    "outputId": "60fd5b1c-233b-41f8-c99a-c7d051170a5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXtlpvN6S2xk"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import random\n",
    "# from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from helpers.model import LSTM_DQN  # using our custom model\n",
    "from helpers.generic import to_np, to_pt, preproc, _words_to_ids, pad_sequences, max_len\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import gym\n",
    "import pdb\n",
    "\n",
    "Transition = namedtuple('Transition', ('observation_id_list', 'v_idx', 'n_idx',\n",
    "                                       'reward', 'mask', 'done', 'is_final', 'observation_str'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity=100000):\n",
    "        # vanilla replay memory\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    #DRQN 获取历史数据\n",
    "    def get_batch(self, batch_size, history_size):\n",
    "        if len(self.memory) <= history_size:\n",
    "            return None\n",
    "        res = []\n",
    "        tried_times = 0\n",
    "        while len(res) < batch_size:\n",
    "            tried_times += 1\n",
    "            if tried_times >= 500:\n",
    "                break\n",
    "            idx = np.random.randint(history_size - 1, len(self.memory) - 1)\n",
    "            # only last frame can be (is_final == True)\n",
    "            if np.any([item.is_final for item in self.memory[idx - (history_size - 1): idx]]):\n",
    "                continue\n",
    "            res.append(self.memory[idx - (history_size - 1): idx + 1])\n",
    "\n",
    "        if len(res) == 0:\n",
    "            return None\n",
    "        res = list(map(list, zip(*res)))  # list (history size) of list (batch) of tuples\n",
    "        return res\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class PrioritizedReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity=100000, priority_fraction=0.0):\n",
    "        # prioritized replay memory\n",
    "        self.priority_fraction = priority_fraction\n",
    "        self.alpha_capacity = int(capacity * priority_fraction)\n",
    "        self.beta_capacity = capacity - self.alpha_capacity\n",
    "        self.alpha_memory, self.beta_memory = [], []\n",
    "        self.alpha_position, self.beta_position = 0, 0\n",
    "\n",
    "    def push(self, is_prior=False, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if is_prior:\n",
    "            if len(self.alpha_memory) < self.alpha_capacity:\n",
    "                self.alpha_memory.append(None)\n",
    "            self.alpha_memory[self.alpha_position] = Transition(*args)\n",
    "            self.alpha_position = (self.alpha_position + 1) % self.alpha_capacity\n",
    "        else:\n",
    "            if len(self.beta_memory) < self.beta_capacity:\n",
    "                self.beta_memory.append(None)\n",
    "            self.beta_memory[self.beta_position] = Transition(*args)\n",
    "            self.beta_position = (self.beta_position + 1) % self.beta_capacity\n",
    "\n",
    "\n",
    "    #DRQN \n",
    "    def _get_batch(self, batch_size, history_size, which_memory):\n",
    "        if len(which_memory) <= history_size:\n",
    "            return None\n",
    "        res = []\n",
    "        tried_times = 0\n",
    "        while len(res) < batch_size:\n",
    "            tried_times += 1\n",
    "            if tried_times >= 500:\n",
    "                break\n",
    "            idx = np.random.randint(history_size - 1, len(which_memory) - 1)\n",
    "            # only last frame can be (is_final == True)\n",
    "            if np.any([item.is_final for item in which_memory[idx - (history_size - 1): idx]]):\n",
    "                continue\n",
    "            res.append(which_memory[idx - (history_size - 1): idx + 1])\n",
    "\n",
    "        if len(res) == 0:\n",
    "            return None\n",
    "        return res\n",
    "    #DRQN \n",
    "    def get_batch(self, batch_size, history_size):\n",
    "        from_alpha = min(int(self.priority_fraction * batch_size), len(self.alpha_memory))\n",
    "        from_beta = min(batch_size - int(self.priority_fraction * batch_size), len(self.beta_memory))\n",
    "        res = []\n",
    "        res_alpha = self._get_batch(from_alpha, history_size, self.alpha_memory)\n",
    "        res_beta = self._get_batch(from_beta, history_size, self.beta_memory)\n",
    "        if res_alpha is None and res_beta is None:\n",
    "            return None\n",
    "        if res_alpha is not None:\n",
    "            res += res_alpha\n",
    "        if res_beta is not None:\n",
    "            res += res_beta\n",
    "        random.shuffle(res)\n",
    "        res = list(map(list, zip(*res)))  # list (history size) of list (batch) of tuples\n",
    "        return res\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.alpha_memory) + len(self.beta_memory)\n",
    "\n",
    "\n",
    "class ObservationHistoryCache(object):\n",
    "\n",
    "    def __init__(self, capacity=1):\n",
    "        # vanilla replay memory\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.reset()\n",
    "\n",
    "    def push(self, stuff):\n",
    "        \"\"\"stuff is list.\"\"\"\n",
    "        for i in range(1, self.capacity):\n",
    "            self.memory[i - 1] = self.memory[i]\n",
    "        self.memory[-1] = stuff\n",
    "\n",
    "    def get_all(self):\n",
    "        res = []\n",
    "        for b in range(len(self.memory[-1])):\n",
    "            tmp = []\n",
    "            for i in range(self.capacity):\n",
    "                if self.memory[i] == []:\n",
    "                    continue\n",
    "                tmp += self.memory[i][b]\n",
    "            res.append(tmp)\n",
    "        return res\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory = []\n",
    "        for i in range(self.capacity):\n",
    "            self.memory.append([])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class RLAgent(object):\n",
    "    def __init__(self, config, word_vocab, verb_map, noun_map, replay_memory_capacity=100000, replay_memory_priority_fraction=0.0, load_pretrained=False):\n",
    "        # print('Creating RL agent...')\n",
    "        self.use_dropout_exploration = True  # TODO: move to config.\n",
    "        self.config = config\n",
    "        self.use_cuda = config['general']['use_cuda']\n",
    "        self.word_vocab = word_vocab\n",
    "        self.verb_map = verb_map\n",
    "        self.noun_map = noun_map\n",
    "        self.word2id = {}\n",
    "        for i, w in enumerate(word_vocab):\n",
    "            self.word2id[w] = i\n",
    "        self.model = LSTM_DQN(model_config=config[\"model\"],\n",
    "                              word_vocab=self.word_vocab,\n",
    "                              verb_map=verb_map,\n",
    "                              noun_map=noun_map,\n",
    "                              enable_cuda=self.use_cuda)\n",
    "        self.action_scorer_hidden_dim = config['model']['lstm_dqn']['action_scorer_hidden_dim']\n",
    "        if load_pretrained:\n",
    "            self.load_pretrained_model(config[\"model\"]['global']['pretrained_model_save_path'])\n",
    "        if self.use_cuda:\n",
    "            self.model.cuda()\n",
    "        if replay_memory_priority_fraction > 0.0:\n",
    "            self.replay_memory = PrioritizedReplayMemory(replay_memory_capacity, priority_fraction=replay_memory_priority_fraction)\n",
    "        else:\n",
    "            self.replay_memory = ReplayMemory(replay_memory_capacity)\n",
    "        self.observation_cache_capacity = config['general']['observation_cache_capacity']\n",
    "        self.observation_cache = ObservationHistoryCache(self.observation_cache_capacity)\n",
    "\n",
    "    def load_pretrained_model(self, load_from):\n",
    "        # load model, if there is any\n",
    "        print(\"loading best model------------------------------------------------------------------\\n\")\n",
    "        try:\n",
    "            save_f = open(load_from, 'rb')\n",
    "            self.model = torch.load(save_f)\n",
    "        except:\n",
    "            print(\"failed...lol\")\n",
    "\n",
    "    def reset(self, infos):\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.intermediate_rewards = []\n",
    "        self.revisit_counting_rewards = []\n",
    "        self.observation_cache.reset()\n",
    "\n",
    "    def get_chosen_strings(self, v_idx, n_idx):\n",
    "        v_idx_np = to_np(v_idx)\n",
    "        n_idx_np = to_np(n_idx)\n",
    "        res_str = []\n",
    "        for i in range(n_idx_np.shape[0]):\n",
    "            v, n = self.verb_map[v_idx_np[i]], self.noun_map[n_idx_np[i]]\n",
    "            res_str.append(self.word_vocab[v] + \" \" + self.word_vocab[n])\n",
    "        return res_str\n",
    "\n",
    "    def choose_random_command(self, verb_rank, noun_rank):\n",
    "        batch_size = verb_rank.size(0)\n",
    "        vr, nr = to_np(verb_rank), to_np(noun_rank)\n",
    "\n",
    "        v_idx, n_idx = [], []\n",
    "        for i in range(batch_size):\n",
    "            v_idx.append(np.random.choice(len(vr[i]), 1)[0])\n",
    "            n_idx.append(np.random.choice(len(nr[i]), 1)[0])\n",
    "        v_qvalue, n_qvalue = [], []\n",
    "        for i in range(batch_size):\n",
    "            v_qvalue.append(verb_rank[i][v_idx[i]])\n",
    "            n_qvalue.append(noun_rank[i][n_idx[i]])\n",
    "        v_qvalue, n_qvalue = torch.stack(v_qvalue), torch.stack(n_qvalue)\n",
    "        v_idx, n_idx = to_pt(np.array(v_idx), self.use_cuda), to_pt(np.array(n_idx), self.use_cuda)\n",
    "        return v_qvalue, v_idx, n_qvalue, n_idx\n",
    "\n",
    "    def choose_maxQ_command(self, verb_rank, noun_rank):\n",
    "        batch_size = verb_rank.size(0)\n",
    "        vr, nr = to_np(verb_rank), to_np(noun_rank)\n",
    "        v_idx = np.argmax(vr, -1)\n",
    "        n_idx = np.argmax(nr, -1)\n",
    "        v_qvalue, n_qvalue = [], []\n",
    "        for i in range(batch_size):\n",
    "            v_qvalue.append(verb_rank[i][v_idx[i]])\n",
    "            n_qvalue.append(noun_rank[i][n_idx[i]])\n",
    "        v_qvalue, n_qvalue = torch.stack(v_qvalue), torch.stack(n_qvalue)\n",
    "        v_idx, n_idx = to_pt(v_idx, self.use_cuda), to_pt(n_idx, self.use_cuda)\n",
    "        return v_qvalue, v_idx, n_qvalue, n_idx\n",
    "\n",
    "    def get_ranks(self, input_description,prev_hidden=None, prev_cell=None):\n",
    "\n",
    "        state_representation = self.model.representation_generator(input_description)\n",
    "        verb_rank, noun_rank, curr_hidden, curr_cell = self.model.recurrent_action_scorer(state_representation, prev_hidden, prev_cell) # batch x n_verb, batch x n_noun\n",
    "        # batch x n_action   # batch x n_object\n",
    "        return verb_rank, noun_rank, curr_hidden, curr_cell\n",
    "\n",
    "    #根据ob 生成 action （最大maxQ）\n",
    "    def generate_one_command(self, input_description, prev_hidden=None, prev_cell=None, epsilon=0.2):\n",
    "        #verb_rank \n",
    "        verb_rank, noun_rank, curr_hidden, curr_cell = self.get_ranks(input_description, prev_hidden, prev_cell)  # batch x n_verb, batch x n_noun\n",
    "        curr_hidden = curr_hidden.detach()\n",
    "        curr_cell = curr_cell.detach()\n",
    "\n",
    "        v_qvalue_maxq, v_idx_maxq, n_qvalue_maxq, n_idx_maxq = self.choose_maxQ_command(verb_rank, noun_rank)\n",
    "        v_qvalue_random, v_idx_random, n_qvalue_random, n_idx_random = self.choose_random_command(verb_rank, noun_rank)\n",
    "\n",
    "        # random number for epsilon greedy\n",
    "        rand_num = np.random.uniform(low=0.0, high=1.0, size=(input_description.size(0),))\n",
    "        less_than_epsilon = (rand_num < epsilon).astype(\"float32\")  # batch  \n",
    "        greater_than_epsilon = 1.0 - less_than_epsilon\n",
    "        less_than_epsilon = to_pt(less_than_epsilon, self.use_cuda, type='float')\n",
    "        greater_than_epsilon = to_pt(greater_than_epsilon, self.use_cuda, type='float')\n",
    "        less_than_epsilon, greater_than_epsilon = less_than_epsilon.long(), greater_than_epsilon.long()\n",
    "        v_idx = less_than_epsilon * v_idx_random + greater_than_epsilon * v_idx_maxq\n",
    "        n_idx = less_than_epsilon * n_idx_random + greater_than_epsilon * n_idx_maxq\n",
    "\n",
    "        v_idx, n_idx = v_idx.detach(), n_idx.detach()\n",
    "\n",
    "        # print(v_idx)\n",
    "        # 转换成str 返回action\n",
    "        chosen_strings = self.get_chosen_strings(v_idx, n_idx)\n",
    "\n",
    "        return v_idx, n_idx, chosen_strings, curr_hidden, curr_cell\n",
    "\n",
    "    def get_game_step_info(self, ob, infos, prev_actions=None):\n",
    "        # concat d/i/q/f/pf together as one string\n",
    "        inventory_strings = infos[\"inventory\"]\n",
    "        inventory_token_list = [preproc(item, str_type='inventory', lower_case=True) for item in inventory_strings]\n",
    "        inventory_id_list = [_words_to_ids(tokens, self.word2id) for tokens in inventory_token_list]\n",
    "\n",
    "        # feedback_strings = [info[\"feedback\"] for info in infos]\n",
    "        feedback_strings = infos[\"feedback\"]\n",
    "        feedback_token_list = [preproc(item, str_type='feedback', lower_case=True) for item in feedback_strings]\n",
    "        feedback_id_list = [_words_to_ids(tokens, self.word2id) for tokens in feedback_token_list]\n",
    "\n",
    "        # quest_strings = [info[\"objective\"] for info in infos]\n",
    "        quest_strings = infos[\"objective\"]\n",
    "        quest_token_list = [preproc(item, str_type='None', lower_case=True) for item in quest_strings]\n",
    "        quest_id_list = [_words_to_ids(tokens, self.word2id) for tokens in quest_token_list]\n",
    "\n",
    "        if prev_actions is not None:\n",
    "            prev_action_token_list = [preproc(item, str_type='None', lower_case=True) for item in prev_actions]\n",
    "            prev_action_id_list = [_words_to_ids(tokens, self.word2id) for tokens in prev_action_token_list]\n",
    "        else:\n",
    "            prev_action_id_list = [[] for _ in infos]\n",
    "\n",
    "        # description_strings = [info[\"description\"] for info in infos]\n",
    "        description_strings = infos[\"description\"]\n",
    "        description_token_list = [preproc(item, str_type='description', lower_case=True) for item in description_strings]\n",
    "        for i, d in enumerate(description_token_list):\n",
    "            if len(d) == 0:\n",
    "                description_token_list[i] = [\"end\"]  # hack here, if empty description, insert word \"end\"\n",
    "        description_id_list = [_words_to_ids(tokens, self.word2id) for tokens in description_token_list]\n",
    "        description_id_list = [_d + _i + _q  + _pa for (_d, _i, _q, _pa) in zip(description_id_list, inventory_id_list, quest_id_list, prev_action_id_list)]\n",
    "        \n",
    "\n",
    "        self.observation_cache.push(description_id_list)\n",
    "        description_with_history_id_list = self.observation_cache.get_all() \n",
    "\n",
    "        input_description = pad_sequences(description_with_history_id_list, maxlen=max_len(description_with_history_id_list), padding='post').astype('int32')\n",
    "        input_description = to_pt(input_description, self.use_cuda)\n",
    "        # pdb.set_trace()\n",
    "        return input_description, description_with_history_id_list\n",
    "\n",
    "    def get_observation_strings(self, infos):\n",
    "        # concat game_id_d/i/d together as one string\n",
    "        game_file_names =  ['cc' for info in infos['game']]\n",
    "        inventory_strings = infos[\"inventory\"]\n",
    "        description_strings = infos[\"description\"]\n",
    "\n",
    "        observation_strings = [_n + _d + _i for (_n, _d, _i) in zip(game_file_names, description_strings, inventory_strings)]\n",
    "\n",
    "        return observation_strings\n",
    "\n",
    "    def compute_reward(self, revisit_counting_lambda=0.0, revisit_counting=True):\n",
    "        if len(self.dones) == 1:\n",
    "            mask = [1.0 for _ in self.dones[-1]]\n",
    "        else:\n",
    "            assert len(self.dones) > 1\n",
    "            mask = [1.0 if not self.dones[-2][i] else 0.0 for i in range(len(self.dones[-1]))]\n",
    "        mask = np.array(mask, dtype='float32')\n",
    "        mask_pt = to_pt(mask, self.use_cuda, type='float')\n",
    "\n",
    "        # self.rewards: list of list, max_game_length x batch_size\n",
    "        rewards = np.array(self.rewards[-1], dtype='float32')  # batch\n",
    "        if revisit_counting:\n",
    "            if len(self.revisit_counting_rewards) > 0:\n",
    "                rewards += np.array(self.revisit_counting_rewards[-1], dtype='float32') * revisit_counting_lambda\n",
    "        rewards_pt = to_pt(rewards, self.use_cuda, type='float')\n",
    "        \n",
    "        # memory mask: play one more step after done\n",
    "        if len(self.dones) < 3:\n",
    "            memory_mask = [1.0 for _ in self.dones[-1]]\n",
    "        else:\n",
    "            memory_mask = [1.0 if mask[i] == 1 or ((not self.dones[-3][i]) and self.dones[-2][i]) else 0.0 for i in range(len(self.dones[-1]))]\n",
    "\n",
    "        return rewards, rewards_pt, mask, mask_pt, memory_mask\n",
    "\n",
    "    def update(self, replay_batch_size, history_size, update_from=0, discount_gamma=0.0):\n",
    "\n",
    "        if len(self.replay_memory) < replay_batch_size:\n",
    "            return None\n",
    "        transitions = self.replay_memory.get_batch(replay_batch_size, history_size + 1)  # list (history_size + 1) of list (batch) of tuples\n",
    "        # last transitions is just for computing the last Q function\n",
    "        if transitions is None:\n",
    "            return None\n",
    "        sequences = [Transition(*zip(*batch)) for batch in transitions]\n",
    "\n",
    "        losses = []\n",
    "        prev_ras_hidden, prev_ras_cell = None, None  # ras: recurrent action scorer\n",
    "        observation_id_list = pad_sequences(sequences[0].observation_id_list, maxlen=max_len(sequences[0].observation_id_list), padding='post').astype('int32')\n",
    "        input_observation = to_pt(observation_id_list, self.use_cuda)\n",
    "        v_idx = torch.stack(sequences[0].v_idx, 0)  # batch x 1\n",
    "        n_idx = torch.stack(sequences[0].n_idx, 0)  # batch x 1\n",
    "        verb_rank, noun_rank, curr_ras_hidden, curr_ras_cell = self.get_ranks(input_observation, prev_ras_hidden, prev_ras_cell)\n",
    "        v_qvalue, n_qvalue = verb_rank.gather(1, v_idx.unsqueeze(-1)).squeeze(-1), noun_rank.gather(1, n_idx.unsqueeze(-1)).squeeze(-1)  # batch\n",
    "        prev_qvalue = torch.mean(torch.stack([v_qvalue, n_qvalue], -1), -1)  # batch\n",
    "        if update_from > 0:\n",
    "            prev_qvalue, curr_ras_hidden, curr_ras_cell = prev_qvalue.detach(), curr_ras_hidden.detach(), curr_ras_cell.detach()\n",
    "\n",
    "        for i in range(1, len(sequences)):\n",
    "            observation_id_list = pad_sequences(sequences[i].observation_id_list, maxlen=max_len(sequences[i].observation_id_list), padding='post').astype('int32')\n",
    "            input_observation = to_pt(observation_id_list, self.use_cuda)\n",
    "            v_idx = torch.stack(sequences[i].v_idx, 0)  # batch x 1\n",
    "            n_idx = torch.stack(sequences[i].n_idx, 0)  # batch x 1\n",
    "\n",
    "            verb_rank, noun_rank, curr_ras_hidden, curr_ras_cell = self.get_ranks(input_observation, curr_ras_hidden, curr_ras_cell)\n",
    "            # max\n",
    "            v_qvalue_max, _, n_qvalue_max, _ = self.choose_maxQ_command(verb_rank, noun_rank)\n",
    "            q_value_max = torch.mean(torch.stack([v_qvalue_max, n_qvalue_max], -1), -1)  # batch\n",
    "            q_value_max = q_value_max.detach()\n",
    "            # from memory\n",
    "            v_qvalue, n_qvalue = verb_rank.gather(1, v_idx.unsqueeze(-1)).squeeze(-1), noun_rank.gather(1, n_idx.unsqueeze(-1)).squeeze(-1)  # batch\n",
    "            q_value = torch.mean(torch.stack([v_qvalue, n_qvalue], -1), -1)  # batch\n",
    "            if i < update_from or i == len(sequences) - 1:\n",
    "                q_value, curr_ras_hidden, curr_ras_cell = q_value.detach(), curr_ras_hidden.detach(), curr_ras_cell.detach()\n",
    "            if i > update_from:\n",
    "                prev_rewards = torch.stack(sequences[i - 1].reward)  # batch\n",
    "                prev_not_done = 1.0 - np.array(sequences[i - 1].done, dtype='float32')  # batch\n",
    "                prev_not_done = to_pt(prev_not_done, self.use_cuda, type='float')\n",
    "                prev_rewards = prev_rewards + prev_not_done * q_value_max * discount_gamma  # batch\n",
    "                prev_mask = torch.stack(sequences[i - 1].mask)  # batch\n",
    "                prev_loss = F.smooth_l1_loss(prev_qvalue * prev_mask, prev_rewards * prev_mask)\n",
    "                losses.append(prev_loss)\n",
    "            prev_qvalue = q_value\n",
    "\n",
    "        return torch.stack(losses).mean()\n",
    "\n",
    "    def finish(self):\n",
    "        # Game has finished.\n",
    "        # this function does nothing, bust compute values that to be printed out\n",
    "        self.final_rewards = np.array(self.rewards[-1], dtype='float32')  # batch\n",
    "        self.final_counting_rewards = np.sum(np.array(self.revisit_counting_rewards), 0)  # batch\n",
    "        dones = []\n",
    "        for d in self.dones:\n",
    "            d = np.array([float(dd) for dd in d], dtype='float32')\n",
    "            dones.append(d)\n",
    "        dones = np.array(dones)\n",
    "        step_used = 1.0 - dones\n",
    "        self.step_used_before_done = np.sum(step_used, 0)  # batch\n",
    "\n",
    "        self.final_intermediate_rewards = []\n",
    "        intermediate_rewards = np.array(self.intermediate_rewards)  # step x batch\n",
    "        intermediate_rewards = np.transpose(intermediate_rewards, (1, 0))  # batch x step\n",
    "        for i in range(intermediate_rewards.shape[0]):\n",
    "            self.final_intermediate_rewards.append(np.sum(intermediate_rewards[i][:int(self.step_used_before_done[i]) + 1]))\n",
    "        self.final_intermediate_rewards = np.array(self.final_intermediate_rewards)\n",
    "\n",
    "    def reset_binarized_counter(self, batch_size):\n",
    "        self.binarized_counter_dict = [{} for _ in range(batch_size)]\n",
    "\n",
    "    def get_binarized_count(self, observation_strings, update=True):\n",
    "        batch_size = len(observation_strings)\n",
    "        count_rewards = []\n",
    "        for i in range(batch_size):\n",
    "            concat_string = observation_strings[i]\n",
    "            if concat_string not in self.binarized_counter_dict[i]:\n",
    "                self.binarized_counter_dict[i][concat_string] = 0.0\n",
    "            if update:\n",
    "                self.binarized_counter_dict[i][concat_string] += 1.0\n",
    "            r = self.binarized_counter_dict[i][concat_string]\n",
    "            r = float(r == 1.0)\n",
    "            count_rewards.append(r)\n",
    "        return count_rewards\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            'model': self.model.state_dict(),\n",
    "            # 'optimizer': self.optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state):\n",
    "        self.model.load_state_dict(state['model'])\n",
    "        # self.optimizer.load_state_dict(state['optimizer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOr89wqJ44Um"
   },
   "source": [
    "## preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1sXp5WKSJgh",
    "outputId": "ab5bdfb0-c86a-4047-836a-afdf06725b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up TextWorld environment...\n",
      "Fished TextWorld environment...\n",
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.append('/content/drive/My Drive/capstone/PaperCode/TextWorld-Coin-Collector-master/gym_textworld')\n",
    "from tensorboardX import SummaryWriter\n",
    "import gym\n",
    "# import gym_textworld   # Register all textworld environments.\n",
    "import textworld\n",
    "import textworld.gym\n",
    "\n",
    "print('Setting up TextWorld environment...')\n",
    "infos_to_request = textworld.EnvInfos(admissible_commands=True, description=True,\n",
    "                                      max_score=True, policy_commands=True,game=True,\n",
    "                                      inventory=True,intermediate_reward=True,score=True,feedback=True,\n",
    "                                      objective=True,command_templates=True,\n",
    "                                      won=True,\n",
    "                                      )\n",
    "\n",
    "gamefiles = [ '/content/drive/My Drive/capstone/tw_games/'+i for i in games_files[:1]]\n",
    "\n",
    "batch_size =len(gamefiles) \n",
    "#register the game  \n",
    "env_id = textworld.gym.register_games(gamefiles,batch_size=batch_size,asynchronous=True,\n",
    "                                      request_infos=infos_to_request,\n",
    "                                      max_episode_steps=50)\n",
    "\n",
    "env = gym.make(env_id)\n",
    "env.seed(config['general']['random_seed'])\n",
    "env.reset()\n",
    "\n",
    "\n",
    "# valid and test env\n",
    "run_test = True\n",
    "if run_test:\n",
    "\n",
    "    infos_to_request = textworld.EnvInfos(admissible_commands=True, description=True,\n",
    "                                        max_score=True, policy_commands=True,game=True,\n",
    "                                        inventory=True,intermediate_reward=True,score=True,feedback=True,\n",
    "                                        objective=True,command_templates=True,\n",
    "                                        won=True,\n",
    "                                        )\n",
    "\n",
    "    gamefiles_test = [ '/content/drive/My Drive/capstone/tw_games_test/'+i for i in TEST_games_files]\n",
    "    batch_size_test =len(gamefiles_test)\n",
    "    #register the game  \n",
    "    env_id_test = textworld.gym.register_games(gamefiles_test,batch_size=batch_size_test,asynchronous=True,\n",
    "                                        request_infos=infos_to_request,\n",
    "                                        max_episode_steps=200)\n",
    "    # Create a Gym environment to play the text game.\n",
    "\n",
    "    env_test = gym.make(env_id_test)\n",
    "    env_test.seed(config['general']['random_seed'])\n",
    "    env_test.reset()\n",
    "\n",
    "print('Fished TextWorld environment...')\n",
    "# Set the random seed manually for reproducibility.\n",
    "np.random.seed(config['general']['random_seed'])\n",
    "torch.manual_seed(config['general']['random_seed'])\n",
    "if torch.cuda.is_available():\n",
    "    if not config['general']['use_cuda']:  \n",
    "        logger.warning(\"WARNING: CUDA device detected but 'use_cuda: false' found in config.yaml\")\n",
    "    else:# ok! cuda\n",
    "        print(\"using cuda\")\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.cuda.manual_seed(config['general']['random_seed'])\n",
    "else:\n",
    "    config['general']['use_cuda'] = False  # Disable CUDA.\n",
    "\n",
    "\n",
    "#DRQN！！！！\n",
    "history_size = config['general']['history_size']\n",
    "update_from = config['general']['update_from']\n",
    "\n",
    "revisit_counting = config['general']['revisit_counting']#True\n",
    "replay_batch_size = config['general']['replay_batch_size']#32\n",
    "replay_memory_capacity = config['general']['replay_memory_capacity']#500000 \n",
    "replay_memory_priority_fraction = config['general']['replay_memory_priority_fraction']# 0.25  # 0.0 to disable this\n",
    "\n",
    "import textworld.text_utils\n",
    "vocab = textworld.text_utils.extract_vocab_from_gamefiles(gamefiles)\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "word2id = {}\n",
    "for i, w in enumerate(vocab):\n",
    "        word2id[w] = i\n",
    "\n",
    "word_vocab=dict2list(word2id)\n",
    "\n",
    "# collect all nouns\n",
    "verb_list = [\"go\", \"take\"]\n",
    "object_name_list = [\"east\", \"west\", \"north\", \"south\", \"coin\"]\n",
    "verb_map = [word2id[w] for w in verb_list if w in word2id]\n",
    "noun_map = [word2id[w] for w in object_name_list if w in word2id]\n",
    "agent = RLAgent(config, word_vocab, verb_map, noun_map,\n",
    "                replay_memory_capacity=replay_memory_capacity, replay_memory_priority_fraction=replay_memory_priority_fraction)\n",
    "\n",
    "\n",
    "exp_dir = get_experiment_dir(config,'twcc_easy_level10_gamesize100','GRU')\n",
    "summary = SummaryWriter(exp_dir)\n",
    "\n",
    "\n",
    "init_learning_rate = config['training']['optimizer']['learning_rate']\n",
    "\n",
    "#换用 优化器\n",
    "parameters = filter(lambda p: p.requires_grad, agent.model.parameters())\n",
    "if config['training']['optimizer']['step_rule'] == 'sgd':\n",
    "    optimizer = torch.optim.SGD(parameters, lr=init_learning_rate)\n",
    "elif config['training']['optimizer']['step_rule'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(parameters, lr=init_learning_rate)\n",
    "\n",
    "\n",
    "log_every = 100\n",
    "reward_avg = SlidingAverage('reward avg', steps=log_every)\n",
    "step_avg = SlidingAverage('step avg', steps=log_every)\n",
    "loss_avg = SlidingAverage('loss avg', steps=log_every)\n",
    "\n",
    "\n",
    "# save & reload checkpoint only in 0th agent\n",
    "best_avg_reward = -10000\n",
    "best_avg_step = 10000\n",
    "\n",
    "# step penalty\n",
    "discount_gamma = config['general']['discount_gamma']\n",
    "provide_prev_action = config['general']['provide_prev_action']\n",
    "\n",
    "# epsilon greedy\n",
    "epsilon_anneal_epochs = config['general']['epsilon_anneal_epochs']\n",
    "epsilon_anneal_from = config['general']['epsilon_anneal_from']\n",
    "epsilon_anneal_to = config['general']['epsilon_anneal_to']\n",
    "\n",
    "# counting reward\n",
    "revisit_counting_lambda_anneal_epochs = config['general']['revisit_counting_lambda_anneal_epochs']\n",
    "revisit_counting_lambda_anneal_from = config['general']['revisit_counting_lambda_anneal_from']\n",
    "revisit_counting_lambda_anneal_to = config['general']['revisit_counting_lambda_anneal_to']\n",
    "\n",
    "epsilon = epsilon_anneal_from\n",
    "revisit_counting_lambda = revisit_counting_lambda_anneal_from\n",
    "\n",
    "bestStep=200\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ch0JUjUyzvH"
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqvv9ufGkc5G"
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "for epoch in range(1,config['training']['scheduling']['epoch']):\n",
    "\n",
    "        agent.model.train()\n",
    "\n",
    "        obs, infos = env.reset()\n",
    "        obs=list(obs)\n",
    "        agent.reset(infos)\n",
    "        print_command_string, print_rewards = [[] for _ in range(batch_size)], [[] for _ in range(batch_size)]  #[[]]\n",
    "        print_interm_rewards = [[] for _ in range(batch_size)]\n",
    "        print_rc_rewards = [[] for _ in range(batch_size)]\n",
    "\n",
    "\n",
    "        dones = [False] * batch_size\n",
    "        rewards = None\n",
    "        avg_loss_in_this_game = []\n",
    "\n",
    "        new_observation_strings = agent.get_observation_strings(infos)\n",
    "\n",
    "        if revisit_counting:\n",
    "            agent.reset_binarized_counter(batch_size)\n",
    "            revisit_counting_rewards = agent.get_binarized_count(new_observation_strings)\n",
    "\n",
    "        current_game_step = 0\n",
    "        prev_actions = [\"\" for _ in range(batch_size)] if provide_prev_action else None  #''\n",
    "\n",
    "        input_description, description_id_list = agent.get_game_step_info(obs, infos, prev_actions)\n",
    "        curr_ras_hidden, curr_ras_cell = None, None  # ras: recurrent action scorer\n",
    "\n",
    "        memory_cache = [[] for _ in range(batch_size)]\n",
    "        solved = [0 for _ in range(batch_size)]\n",
    "\n",
    "        while not all(dones):\n",
    "            agent.model.train()\n",
    "            v_idx, n_idx, chosen_strings, curr_ras_hidden, curr_ras_cell = agent.generate_one_command(input_description, curr_ras_hidden, curr_ras_cell, epsilon=epsilon)\n",
    "\n",
    "\n",
    "            obs, rewards, dones, infos = env.step(chosen_strings) \n",
    "            obs=list(obs)\n",
    "            rewards=list(rewards)\n",
    "            dones=list(dones)\n",
    "            \n",
    "            curr_observation_strings = agent.get_observation_strings(infos)\n",
    "\n",
    "            if provide_prev_action:\n",
    "                prev_actions = chosen_strings\n",
    "            # counting\n",
    "            if revisit_counting:\n",
    "                revisit_counting_rewards = agent.get_binarized_count(curr_observation_strings, update=True)\n",
    "            else:\n",
    "                revisit_counting_rewards = [0.0 for _ in range(batch_size)]\n",
    "            agent.revisit_counting_rewards.append(revisit_counting_rewards)\n",
    "            revisit_counting_rewards = [float(format(item, \".3f\")) for item in revisit_counting_rewards]\n",
    "\n",
    "            for i in range(len(infos['game'])):\n",
    "                print_command_string[i].append(chosen_strings[i])\n",
    "                print_rewards[i].append(rewards[i])\n",
    "                print_interm_rewards[i].append(infos[\"intermediate_reward\"][i])\n",
    "                print_rc_rewards[i].append(revisit_counting_rewards[i])\n",
    "            if type(dones) is bool:\n",
    "                dones = [dones] * batch_size\n",
    "            agent.rewards.append(rewards)\n",
    "            agent.dones.append(dones)\n",
    "            agent.intermediate_rewards.append([info for info in infos[\"intermediate_reward\"]])\n",
    "\n",
    "            # computer rewards, and push into replay memory\n",
    "            rewards_np, rewards_pt, mask_np, mask_pt, memory_mask = agent.compute_reward(revisit_counting_lambda=revisit_counting_lambda, revisit_counting=revisit_counting)\n",
    "\n",
    "           \n",
    "            curr_description_id_list = description_id_list\n",
    "            input_description, description_id_list = agent.get_game_step_info(obs, infos, prev_actions)\n",
    "            \n",
    "            for b in range(batch_size):\n",
    "                if memory_mask[b] == 0:\n",
    "                    continue\n",
    "                if dones[b] == 1 and rewards[b] == 0:\n",
    "                    # last possible step\n",
    "                    is_final = True\n",
    "                else:\n",
    "                    is_final = mask_np[b] == 0\n",
    "                if rewards[b] > 0.0:\n",
    "                    solved[b] = 1\n",
    "                # replay memory\n",
    "                memory_cache[b].append((curr_description_id_list[b], v_idx[b], n_idx[b], rewards_pt[b], mask_pt[b], dones[b], is_final, curr_observation_strings[b]))\n",
    "\n",
    "\n",
    "            \n",
    "            if current_game_step > 0 and current_game_step % config[\"general\"][\"update_per_k_game_steps\"] == 0:  # update_per_k_game_steps: 4\n",
    "                policy_loss = agent.update(replay_batch_size, history_size, update_from, discount_gamma=discount_gamma)\n",
    "                \n",
    "                if policy_loss is None:\n",
    "                    continue\n",
    "                loss = policy_loss\n",
    "                # Backpropagate\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "                # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "                torch.nn.utils.clip_grad_norm_(agent.model.parameters(), config['training']['optimizer']['clip_grad_norm'])\n",
    "                optimizer.step()  # apply gradients\n",
    "                avg_loss_in_this_game.append(to_np(policy_loss))\n",
    "            current_game_step += 1\n",
    "        \n",
    "        \n",
    "        for i, mc in enumerate(memory_cache):\n",
    "            for item in mc:\n",
    "                if replay_memory_priority_fraction == 0.0:\n",
    "                    # vanilla replay memory\n",
    "                    agent.replay_memory.push(*item)\n",
    "                else:\n",
    "                    # prioritized replay memory\n",
    "                    agent.replay_memory.push(solved[i], *item)\n",
    "\n",
    "\n",
    "        agent.finish()\n",
    "        avg_loss_in_this_game = np.mean(avg_loss_in_this_game)\n",
    "        reward_avg.add(agent.final_rewards.mean())\n",
    "        step_avg.add(agent.step_used_before_done.mean())\n",
    "        loss_avg.add(avg_loss_in_this_game)\n",
    "        # annealing\n",
    "        if epoch < epsilon_anneal_epochs:\n",
    "            epsilon -= (epsilon_anneal_from - epsilon_anneal_to) / float(epsilon_anneal_epochs)\n",
    "        if epoch < revisit_counting_lambda_anneal_epochs:\n",
    "            revisit_counting_lambda -= (revisit_counting_lambda_anneal_from - revisit_counting_lambda_anneal_to) / float(revisit_counting_lambda_anneal_epochs)\n",
    "\n",
    "        # Tensorboard logging #\n",
    "        # (1) Log some numbers\n",
    "        if (epoch + 1) % config[\"training\"][\"scheduling\"][\"logging_frequency\"] == 0:\n",
    "            summary.add_scalar('avg_reward', reward_avg.value, epoch + 1)\n",
    "            summary.add_scalar('curr_reward', agent.final_rewards.mean(), epoch + 1)\n",
    "            summary.add_scalar('curr_interm_reward', agent.final_intermediate_rewards.mean(), epoch + 1)\n",
    "            summary.add_scalar('curr_counting_reward', agent.final_counting_rewards.mean(), epoch + 1)\n",
    "            summary.add_scalar('avg_step', step_avg.value, epoch + 1)\n",
    "            summary.add_scalar('curr_step', agent.step_used_before_done.mean(), epoch + 1)\n",
    "            summary.add_scalar('loss_avg', loss_avg.value, epoch + 1)\n",
    "            summary.add_scalar('curr_loss', avg_loss_in_this_game, epoch + 1)\n",
    "\n",
    "        msg = 'E#{:03d}, R={:.3f}/{:.3f}/IR{:.3f}/CR{:.3f}, S={:.3f}/{:.3f}, L={:.6f}/{:.3f}, epsilon={:.4f}, lambda_counting={:.4f}'\n",
    "        msg = msg.format(epoch,\n",
    "                         np.mean(reward_avg.value), agent.final_rewards.mean(), agent.final_intermediate_rewards.mean(), agent.final_counting_rewards.mean(),\n",
    "                         np.mean(step_avg.value), agent.step_used_before_done.mean(),\n",
    "                         np.mean(loss_avg.value), avg_loss_in_this_game,\n",
    "                         epsilon, revisit_counting_lambda)\n",
    "        if (epoch + 1) % config[\"training\"][\"scheduling\"][\"logging_frequency\"] == 0:\n",
    "            print(\"=========================================================\")\n",
    "            # for prt_cmd, prt_rew, prt_int_rew, prt_rc_rew in zip(print_command_string, print_rewards, print_interm_rewards, print_rc_rewards):\n",
    "            #     print(\"------------------------------\")\n",
    "            #     print(prt_cmd)\n",
    "            #     print(prt_rew)\n",
    "            #     print(prt_int_rew)\n",
    "            #     print(prt_rc_rew)\n",
    "        print(msg)\n",
    "        # test on a different set of games\n",
    "        if run_test and (epoch + 1) % config[\"training\"][\"scheduling\"][\"logging_frequency\"] == 0:\n",
    "\n",
    "            # agent.model.eval()\n",
    "            print('*********** Valid part: ************')\n",
    "            valid_R, valid_IR, valid_S = testing(env,batch_size,agent)\n",
    "            summary.add_scalar('valid_reward', valid_R, epoch + 1)\n",
    "            summary.add_scalar('valid_interm_reward', valid_IR, epoch + 1)\n",
    "            summary.add_scalar('valid_step', valid_S, epoch + 1)\n",
    "\n",
    "\n",
    "            print('*********** Test part: ************')\n",
    "            # save & reload checkpoint by best valid performance\n",
    "            R, IR, S=testing(env_test,batch_size_test,agent)   \n",
    "            summary.add_scalar('test_reward_', R, epoch + 1)\n",
    "            summary.add_scalar('test_interm_reward_', IR, epoch + 1)\n",
    "            summary.add_scalar('test_step_', S, epoch + 1)\n",
    "            summary.close()\n",
    "            \n",
    "            if bestStep>S or bestStep>valid_S:\n",
    "                PATH='/content/drive/My Drive/capstone/checkpoint/GRU-DRQN_level10_100multiGames'\n",
    "                torch.save(agent.state_dict(), PATH)\n",
    "                torch.save(optimizer.state_dict(), PATH+'_opt')\n",
    "                bestStep=S\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GRU-DRQN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
